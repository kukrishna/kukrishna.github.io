<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>CNNv2</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>


<style type="text/css">
body
{
  font-size: 1.8em; 
}
</style>

</head>
<body><div class="container"><h4 id="back"><a href="../index.html"><i class="icon-angle-double-left"></i> Back</a></h4>

<h1 id="what-does-your-neural-network-really-look-for">What does your neural network really look for?</h1>

<p>When I first read about neural networks, it made an impression of a big bulky box with thousands of parameters who would just munch on any and all kinds of data under the Sun and magically pop out the state of the art  accuracy score.  I felt bad that all the carefully engineered algorithms would fall behind them in performance. It felt like a shortcut, wherein instead of researching deep into the problem, people just use nets to get the “best” results and publish papers.</p>

<p>Until recently, when I came across Jason Yosinki’s work<a href="#fn:yos" id="fnref:yos" title="See footnote" class="footnote">1</a> where they try to reconstruct images of particular classes from trained Convolutional Neural Networks. Following this thread, I found a long string of work by many researchers aimed to go beyond the accuracies and F1-scores to find what these CNNs want to see in the image to give a confident classification. There is an interesting (and maybe humorous) anecdote   <a href="https://neil.fraser.name/writing/tank/"  target="_blank">here</a>  on why you should be interested in doing so.</p>

<p>How difficult is it to discriminate between images? In this blogpost, I have tried to explore this question. </p>

<p>This blog is actually the markdown view of an IPython notebook. You can download this entire page (both the code and text by downloading the notebook <a href="resources/mnistBlog.ipynb"  target="_blank">here</a>). You will need Jupyter<a href="#fn:jup" id="fnref:jup" title="See footnote" class="footnote">2</a> to open and run the notebook. </p>

<p>First we’ll setup our workbench using Lasagne and theano. We will be using the famous MNIST dataset for our explorations.</p>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> theano
<span class="hljs-keyword">import</span> gzip
<span class="hljs-keyword">import</span> cPickle
<span class="hljs-keyword">import</span> PIL
<span class="hljs-keyword">from</span> scipy.misc <span class="hljs-keyword">import</span> bytescale
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> lasagne.init <span class="hljs-keyword">import</span> Constant, GlorotUniform
<span class="hljs-keyword">import</span> sys

theano.config.floatX=<span class="hljs-string">'float32'</span>
theano.config.optimizer=<span class="hljs-string">'fast_run'</span>
theano.config.exception_verbosity=<span class="hljs-string">'high'</span></code></pre>

<p>To view images inline in IPython notebook, I’m using the reference from the top answer here. </p>

<p><a href="http://stackoverflow.com/questions/8409143/quickest-way-to-preview-a-pil-image"  target="_blank">http://stackoverflow.com/questions/8409143/quickest-way-to-preview-a-pil-image</a></p>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># display_pil.py</span>
<span class="hljs-comment"># source: http://mail.scipy.org/pipermail/ipython-user/2012-March/009706.html</span>
<span class="hljs-comment"># by 'MinRK'</span>
<span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> IPython.core <span class="hljs-keyword">import</span> display
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">display_pil_image</span><span class="hljs-params">(im)</span>:</span>
    <span class="hljs-string">"""displayhook function for PIL Images, rendered as PNG"""</span>
    b = BytesIO()
    im.save(b, format=<span class="hljs-string">'png'</span>)
    data = b.getvalue()

    ip_img = display.Image(data=data, format=<span class="hljs-string">'png'</span>, embed=<span class="hljs-keyword">True</span>)
    <span class="hljs-keyword">return</span> ip_img._repr_png_()

<span class="hljs-comment"># register display func with PNG formatter:</span>
png_formatter = get_ipython().display_formatter.formatters[<span class="hljs-string">'image/png'</span>]
png_formatter.for_type(Image.Image, display_pil_image)</code></pre>

<p>The length of this post seems intimidating because of the amount of code here. I would advise you to skip the section on the training code - it exists only for the sake of completeness for people who would like to run this experiment. </p>

<p><div class="toc">
<ul>
<li><a href="what-does-your-neural-network-really-look-for">What does your neural network really look for?</a></li>
<li><a href="#the-code-skip-it">The Code (skip it?)</a><ul>
<li><a href="#choosing-two-similar-looking-numbers">Choosing two similar looking numbers</a></li>
<li><a href="#training-the-network">Training the network</a></li>
</ul>
</li>
<li><a href="#the-results">The Results</a><ul>
<li><a href="#testing-out-the-model-on-hand-drawn-input">Testing out the model on hand drawn input</a></li>
<li><a href="#why-is-this-bad">Why is this bad?</a></li>
<li><a href="#too-much-expectations-from-machine">Too much expectations from machine</a></li>
<li><a href="#discriminative-vs-generative-learning-teaching-machines-to-imagine">Discriminative vs Generative learning : Teaching machines to imagine</a></li>
<li><a href="#in-which-way-do-we-humans-learn">In which way do we humans learn?</a></li>
<li><a href="#beyond-classification">Beyond classification</a></li>
</ul>
</li>
</ul>
</div>
</p>



<h1 id="the-code-skip-it">The Code (skip it?)</h1>



<h2 id="choosing-two-similar-looking-numbers">Choosing two similar looking numbers</h2>

<p>We load the MNIST dataset.</p>



<pre class="prettyprint"><code class="language-python hljs ">f=gzip.open(<span class="hljs-string">"./mnist.pkl.gz"</span>)
mnistdata=cPickle.load(f)
train_set, valid_set, test_set = mnistdata</code></pre>

<p>Take the numbers 1 and 7. The differentiating factor amongst the two is that seven will have well pronounced horizontal strokes on top, and maybe an extra on in the middle. While both 1 and 7 have a roughly vertical stroke, the distinguishing factor has to be the horizontal strokes. Let’s pick these two from the dataset.</p>



<pre class="prettyprint"><code class="language-python hljs ">allones=train_set[<span class="hljs-number">0</span>][np.where(train_set[<span class="hljs-number">1</span>]==<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]]
allsevens=train_set[<span class="hljs-number">0</span>][np.where(train_set[<span class="hljs-number">1</span>]==<span class="hljs-number">7</span>)[<span class="hljs-number">0</span>]]</code></pre>

<p>I would like to get an overview of how people write 1s and 7s before proceeding. These helper functions tile to make a large image from list of digit images.</p>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tile</span><span class="hljs-params">(x,m,n)</span>:</span>
    y=x.copy().reshape((m,n,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>))
    <span class="hljs-keyword">return</span> np.hstack(np.hstack(y))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getImage</span><span class="hljs-params">(x)</span>:</span>
    im=bytescale(x)
    <span class="hljs-keyword">return</span> PIL.Image.fromarray(im)</code></pre>



<pre class="prettyprint"><code class="language-python hljs ">getImage(
    np.hstack(( tile(allsevens[np.random.permutation(allsevens.shape[<span class="hljs-number">0</span>])[<span class="hljs-number">0</span>:<span class="hljs-number">400</span>]],<span class="hljs-number">20</span>,<span class="hljs-number">20</span>) , 
               tile(allones[np.random.permutation(allones.shape[<span class="hljs-number">0</span>])[<span class="hljs-number">0</span>:<span class="hljs-number">400</span>]],<span class="hljs-number">20</span>,<span class="hljs-number">20</span>)  )) 
        )</code></pre>

<p><img src="output_14_0.png" alt="png" title=""></p>

<p>Looks satisfactory. All 7s have a horizontal stroke on top, while only a few 1s have such a horizontal stroke in the same position.  Now my goal is to see that if I train a neural net to differentiate between them, then does it so happen, that if there is a horizontal line, the net says 7 and otherwise it says 1.</p>



<h2 id="training-the-network">Training the network</h2>

<p>What follows next is boring code for preparing the data for binary classification.</p>



<pre class="prettyprint"><code class="language-python hljs ">numSamples=<span class="hljs-number">5000</span>

permkey1=np.random.permutation(<span class="hljs-number">5000</span>)[<span class="hljs-number">0</span>:numSamples]
numTrain=<span class="hljs-number">4000</span>
trainXs=np.concatenate((allones[permkey1][:numTrain],allsevens[permkey1][:numTrain]))
trainYs=np.concatenate((    np.zeros((numTrain))  ,  np.ones((numTrain))   ))

testXs=np.concatenate((allones[permkey1][numTrain:],allsevens[permkey1][numTrain:]))
testYs=np.concatenate((    np.zeros((numSamples - numTrain))  ,  np.ones((numSamples - numTrain))   ))</code></pre>

<p>Now we come to designing the network. The design has been taken from ColinRaffael’s online tutorial. Except that I’ve not used dropout. </p>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> lasagne
<span class="hljs-keyword">import</span> theano.tensor <span class="hljs-keyword">as</span> T 

ConvLayer=lasagne.layers.Conv2DLayer

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getNet</span><span class="hljs-params">()</span>:</span>
    net={}
    net[<span class="hljs-string">'input_var'</span>]=T.tensor4()
    net[<span class="hljs-string">'inputlayer'</span>]=lasagne.layers.InputLayer((<span class="hljs-keyword">None</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>),input_var=net[<span class="hljs-string">'input_var'</span>])
    net[<span class="hljs-string">'conv1'</span>]=ConvLayer(net[<span class="hljs-string">'inputlayer'</span>],<span class="hljs-number">20</span>,(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>),stride=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), pad=<span class="hljs-string">'valid'</span>, nonlinearity=lasagne.nonlinearities.rectify,
                          W=GlorotUniform(<span class="hljs-string">'relu'</span>), b=Constant(<span class="hljs-number">0.0</span>))
    net[<span class="hljs-string">'pool1'</span>]=lasagne.layers.Pool2DLayer(net[<span class="hljs-string">'conv1'</span>],<span class="hljs-number">2</span>)
    net[<span class="hljs-string">'conv2'</span>]=ConvLayer(net[<span class="hljs-string">'pool1'</span>],<span class="hljs-number">50</span>,(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>),stride=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), pad=<span class="hljs-string">'valid'</span>, nonlinearity=lasagne.nonlinearities.rectify,
                          W=GlorotUniform(<span class="hljs-string">'relu'</span>), b=Constant(<span class="hljs-number">0.0</span>))
    net[<span class="hljs-string">'pool2'</span>]=lasagne.layers.Pool2DLayer(net[<span class="hljs-string">'conv2'</span>],<span class="hljs-number">2</span>)
    net[<span class="hljs-string">'flattened'</span>]=lasagne.layers.FlattenLayer(net[<span class="hljs-string">'pool2'</span>])
    net[<span class="hljs-string">'dense1'</span>]=lasagne.layers.DenseLayer(net[<span class="hljs-string">'flattened'</span>], num_units=<span class="hljs-number">256</span>, nonlinearity=lasagne.nonlinearities.rectify,
                                           W=GlorotUniform(<span class="hljs-string">'relu'</span>), b=Constant(<span class="hljs-number">0.0</span>))
    net[<span class="hljs-string">'dense2'</span>]=lasagne.layers.DenseLayer(net[<span class="hljs-string">'dense1'</span>], num_units=<span class="hljs-number">2</span>, nonlinearity=lasagne.nonlinearities.softmax,
                                           W=GlorotUniform() )
    net[<span class="hljs-string">'output'</span>]=net[<span class="hljs-string">'dense2'</span>]
    <span class="hljs-keyword">return</span> net</code></pre>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#code modified form of ColinRafaels's code</span>

cnn=getNet()

true_output = T.imatrix(<span class="hljs-string">'true_output'</span>)
netoutput=lasagne.layers.get_output(cnn[<span class="hljs-string">'output'</span>])
loss_train = lasagne.objectives.categorical_crossentropy(netoutput, true_output)

all_params = lasagne.layers.get_all_params(cnn[<span class="hljs-string">'output'</span>])

aggregate_loss=lasagne.objectives.aggregate(loss_train)

<span class="hljs-comment"># Use ADADELTA for updates</span>
updates = lasagne.updates.adadelta(aggregate_loss, all_params)

train = theano.function([cnn[<span class="hljs-string">'input_var'</span>], true_output], loss_train, updates=updates)
<span class="hljs-comment"># This is the function we'll use to compute the network's output given an input</span>
<span class="hljs-comment"># (e.g., for computing accuracy).</span>
get_output = theano.function([cnn[<span class="hljs-string">'input_var'</span>]], netoutput)</code></pre>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># changing my data's tensor shapes</span>
traindict={}
permkey2=np.random.permutation(trainXs.shape[<span class="hljs-number">0</span>])
traindict[<span class="hljs-string">'X'</span>]=trainXs[permkey2].reshape(trainXs.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)
traindict[<span class="hljs-string">'y'</span>]=trainYs[permkey2].astype(<span class="hljs-string">'int32'</span>)
traindict[<span class="hljs-string">'y'</span>]=np.asarray(map(<span class="hljs-keyword">lambda</span> q: np.array([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],dtype=<span class="hljs-string">'int32'</span>) <span class="hljs-keyword">if</span> q==<span class="hljs-number">1</span> <span class="hljs-keyword">else</span>  np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],dtype=<span class="hljs-string">'int32'</span>) 
                              ,traindict[<span class="hljs-string">'y'</span>]))

validdict={}
validdict[<span class="hljs-string">'X'</span>]=testXs.reshape(testXs.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)
validdict[<span class="hljs-string">'y'</span>]=testYs.astype(<span class="hljs-string">'int32'</span>)
validdict[<span class="hljs-string">'y'</span>]=np.asarray(map(<span class="hljs-keyword">lambda</span> q: np.array([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],dtype=<span class="hljs-string">'int32'</span>) <span class="hljs-keyword">if</span> q==<span class="hljs-number">1</span> <span class="hljs-keyword">else</span>  np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],dtype=<span class="hljs-string">'int32'</span>) 
                              ,validdict[<span class="hljs-string">'y'</span>]))

dataset={}
dataset[<span class="hljs-string">'train'</span>]=traindict
dataset[<span class="hljs-string">'valid'</span>]=validdict</code></pre>

<p>We’ll now train the network in batches of 100 images. 8 epochs would be enough.</p>



<pre class="prettyprint"><code class="language-python hljs ">
BATCH_SIZE = <span class="hljs-number">100</span>
N_EPOCHS = <span class="hljs-number">8</span>
<span class="hljs-comment"># Keep track of which batch we're training with</span>
batch_idx = <span class="hljs-number">0</span>
<span class="hljs-comment"># Keep track of which epoch we're on</span>
epoch = <span class="hljs-number">0</span>

print(<span class="hljs-string">"starting the first epoch"</span>)
sys.stdout.flush()

<span class="hljs-keyword">while</span> epoch &lt; N_EPOCHS:
    <span class="hljs-comment"># Extract the training data/label batch and update the parameters with it</span>
    train(dataset[<span class="hljs-string">'train'</span>][<span class="hljs-string">'X'</span>][batch_idx:batch_idx + BATCH_SIZE],
                     dataset[<span class="hljs-string">'train'</span>][<span class="hljs-string">'y'</span>][batch_idx:batch_idx + BATCH_SIZE])
    batch_idx += BATCH_SIZE
    <span class="hljs-comment"># Once we've trained on the entire training set...</span>
    <span class="hljs-keyword">if</span> batch_idx &gt;= dataset[<span class="hljs-string">'train'</span>][<span class="hljs-string">'X'</span>].shape[<span class="hljs-number">0</span>]:
        <span class="hljs-comment"># Reset the batch index</span>
        batch_idx = <span class="hljs-number">0</span>
        <span class="hljs-comment"># Update the number of epochs trained</span>
        epoch += <span class="hljs-number">1</span>
        <span class="hljs-comment"># Compute the network's output on the validation data</span>
        val_output = get_output(dataset[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'X'</span>])
        <span class="hljs-comment"># The predicted class is just the index of the largest probability in the output</span>
        val_predictions = (val_output&gt;<span class="hljs-number">0.5</span>).astype(<span class="hljs-string">"int32"</span>)
        <span class="hljs-comment"># The accuracy is the average number of correct predictions</span>
        accuracy = np.mean(val_predictions == dataset[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'y'</span>])
        print(<span class="hljs-string">"Epoch {} validation accuracy: {}"</span>.format(epoch, accuracy))</code></pre>

<pre><code>starting the first epoch
Epoch 1 validation accuracy: 0.9945
Epoch 2 validation accuracy: 0.995
Epoch 3 validation accuracy: 0.994
Epoch 4 validation accuracy: 0.9945
Epoch 5 validation accuracy: 0.9945
Epoch 6 validation accuracy: 0.9945
Epoch 7 validation accuracy: 0.9945
Epoch 8 validation accuracy: 0.9945
</code></pre>



<pre class="prettyprint"><code class="language-python hljs ">val_output = get_output(dataset[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'X'</span>])
val_predictions=(val_output&gt;<span class="hljs-number">0.5</span>).astype(<span class="hljs-string">'int32'</span>)
<span class="hljs-comment"># The accuracy is the average number of correct predictions</span>
accuracy = np.mean(val_predictions == dataset[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'y'</span>])
print(<span class="hljs-string">"Validation accuracy: {}"</span>.format( accuracy))</code></pre>

<pre><code>Validation accuracy: 0.9945
</code></pre>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#Saving the model for loading later</span>
<span class="hljs-keyword">import</span> pickle
trainedparamvals=map(<span class="hljs-keyword">lambda</span> q:q.eval(),all_params)
pickle.dump(trainedparamvals,open(<span class="hljs-string">"trainedparamvals.pkl"</span>,<span class="hljs-string">"wb"</span>))</code></pre>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#If you have the weight file, you don't need to train the network.You can just load the weights from the disk</span>
<span class="hljs-keyword">import</span> pickle
trainedparamvals=pickle.load(open(<span class="hljs-string">"./trainedparamvals.pkl"</span>,<span class="hljs-string">"rb"</span>))
<span class="hljs-keyword">for</span> i,j <span class="hljs-keyword">in</span> zip(all_params,trainedparamvals):
    i.set_value(j)</code></pre>



<h1 id="the-results">The Results</h1>



<h2 id="testing-out-the-model-on-hand-drawn-input">Testing out the model on hand drawn input</h2>

<p>I’ll now feed some hand drawn inputs to the network and see how well it does on these.</p>



<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#Helper function for transforming opened image and feeding it into the net </span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getPred</span><span class="hljs-params">(foolingImage)</span>:</span>
    foolingImageArr=np.array(foolingImage)
    foolingImageArr=foolingImageArr.reshape(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)/foolingImageArr.max()
    print(get_output(foolingImageArr))</code></pre>

<p>Let’s start with a normal 1.</p>



<pre class="prettyprint"><code class="language-python hljs ">foolingImage=PIL.Image.open(<span class="hljs-string">"./normal1.jpg"</span>)
getPred(foolingImage)
foolingImage</code></pre>

<pre><code>[[ 0.94842285  0.05157717]]
</code></pre>

<p><img src="output_31_1.png" alt="png" title=""></p>

<p>The left number is the probability of being a 1 and the right one of being a 7. It clearly did well. Now a 7.</p>



<pre class="prettyprint"><code class="language-python hljs ">foolingImage=PIL.Image.open(<span class="hljs-string">"./normal7.jpg"</span>)
getPred(foolingImage)
foolingImage</code></pre>

<pre><code>[[ 0.00326309  0.99673688]]
</code></pre>

<p><img src="output_33_1.png" alt="png" title=""></p>

<p>Everything seems right. Now let’s feed in some weird data - a stright horizontal stroke.</p>



<pre class="prettyprint"><code class="language-python hljs ">foolingImage=PIL.Image.open(<span class="hljs-string">"./fool7.jpg"</span>)
getPred(foolingImage)
foolingImage</code></pre>

<pre><code>[[  3.00072570e-04   9.99699950e-01]]
</code></pre>

<p><img src="output_35_1.png" alt="png" title=""></p>

<p>The network is overwhelmingly sure that the image is a 7. It seems that the network has learnt a simple way to score good in the tests. If it sees a horizontal stroke, it will say 7, because all 7s have horizontal strokes. Therefore it is very sure about its prediction. We obviously know that’s not 7. So the notion of a 7 that the network has learnt is way different from how we humans perceive a 7. It even gets worse with the next test image.  </p>



<pre class="prettyprint"><code class="language-python hljs ">foolingImage=PIL.Image.open(<span class="hljs-string">"./fool1.jpg"</span>)
getPred(foolingImage)
foolingImage</code></pre>

<pre><code>[[  9.99794364e-01   2.05609627e-04]]
</code></pre>

<p><img src="output_37_1.png" alt="png" title=""></p>

<p>As far as humans are concerned, we would classify the previous two as the same “symbol”. However, the network learns that some 1s have a horizontal base stroke at the bottom and thinks that this is most probably a 1. </p>

<p>One of the principal arguments in support of CNNs is that they are able to detect objects even when they are translated. The next two images are 1s with the exact same pixel values, which are just translated. </p>



<pre class="prettyprint"><code class="language-python hljs ">foolingImage=PIL.Image.open(<span class="hljs-string">"./shift1.jpg"</span>)
getPred(foolingImage)
foolingImage</code></pre>

<pre><code>[[ 0.05307976  0.94692022]]
</code></pre>

<p><img src="output_39_1.png" alt="png" title=""></p>



<pre class="prettyprint"><code class="language-python hljs ">foolingImage=PIL.Image.open(<span class="hljs-string">"./shift2.jpg"</span>)
getPred(foolingImage)
foolingImage</code></pre>

<pre><code>[[ 0.98019415  0.01980587]]
</code></pre>

<p><img src="output_40_1.png" alt="png" title=""></p>

<p>One of them is classified as 7. So has the network assumed that while writing a 1, most people would start from the right side? That is something that we definitely donot want it to learn. Although I think this might be solved by data augmentation where you add randomly shifted copies of the original training images to the training data. </p>



<h2 id="why-is-this-bad">Why is this bad?</h2>

<p>Although data augmentation can take care of the different answers we are getting here by just shifting the input, it is still feels like a manual hack. Our perception of a seven might not have been encoded into it, and it might have settled at a good hack which can help it score well. At that time, the classification score will be great. But does the network think about a 7 the same way we do? Maybe not. It might not have learnt the characteristic way we look at the 7 - as  a collection to two strokes, the way we have been taught to write in our childhood. For the network, a dash (-) and a seven (7) are the same. I think it’s important that after training, networks look at the world like we do.</p>


<h2 id="too-much-expectations-from-machine">Too much expectation from machine</h2>

<p>Before we start criticizing the network, I’d like to make a case in its defence. The only thing that this machine has seen in this world is 1s and 7s. We know that a dash(-) is not the same as a seven(7) because we have encountered moments in our life where we have seen and used both symbols in different contexts. Think about it. If there were no digit as 7, but we were familiar with the dash(-) symbol, you would have considered 7 as a dash(-) with a vertical smeared scratch (which we would discard as noise). </p>



<h2 id="discriminative-vs-generative-learning-teaching-machines-to-imagine">Discriminative vs Generative learning : Teaching machines to imagine</h2>

<p>Discriminative learning aims at classifying inputs into different classes. The aim of generative learning, on the other hand, is to learn to produce instances of a particular class. So a discriminative model like what we’ve trained here is good at classifying images, <strong>provided that they actually belong to one of the classes</strong> that the model was trained to recognize. The life of generative models is much harder. If you ask a machine to draw a seven, it has a completely empty canvas infront of it which it has to paint. While a discriminative model can get away with learning a hack (like this one did) to get good accuracy, the generative model has to struggle to get an overall sense of what a seven(7) should look like. They have to learn to imagine. </p>



<h2 id="in-which-way-do-we-humans-learn">In which way do humans learn?</h2>

<p>We are indeed able to imagine up instances of any particular class(object) in our minds. And so one would expect that our brains learn in a generative fashion and does not engage in shortcuts.</p>

<blockquote>
  <p>“LOL” <br>
        <i>-your brain</i></p>
</blockquote>

<p>Well it turns out that the brain is lazy. It will also at times employ hacks. Remember how you have trouble differentiating between faces of other ethnicity. The cool thing is that they have problem differentiating between faces of your ethnicity. This is a fairly well documented phenomenon in psychology called the <a href="http://study.com/academy/lesson/outgroup-homogeneity-definition-effects-quiz.html"  target="_blank">Outgroup homogeneity effect</a>. This suggests that the brain would like to look at those features of the face which vary the most and allow it to differentiate between people. Since you mostly see people of your own ethnicity around you, you’ll learn to classify based on those features (like eyes). But when you get faces from a different ethnicity, these features might not vary as much, and you’ll have trouble differentiating between them.</p>

<p>We can dream as well as and classify. This two way process of classification, feature learning and using them back for input recreation is something that we should aim for our AI models.</p>



<h2 id="beyond-classification">Beyond classification</h2>

<p>If all we teach our models is classification, then I doubt if we’ll be able to make human like systems. I feel that instead of making machines achieve 99.9999% accuracy at one particular task, we should focus on making them work more like our brains (which actually often makes mistakes). For example, we should think why did we learn to imagine? I don’t know the exact answer to this question. Maybe to anticipate the future and optimize actions to maximize some expected rewards (Google GO, anyone?). What is the task that would help machines learn that?  Reinforncement learning shows the way to model more complex tasks and loss functions, which is a good step in the direction of giving machines more “human-like” learning environment.</p>




<div class="footnotes"><hr><ol><li id="fn:yos"><a href="http://yosinski.com/deepvis"  target="_blank">http://yosinski.com/deepvis</a> <a href="#fnref:yos" title="Return to article" class="reversefootnote">↩</a></li><li id="fn:jup"><a href="http://jupyter.org/"  target="_blank">http://jupyter.org/</a> <a href="#fnref:jup" title="Return to article" class="reversefootnote">↩</a></li></ol></div>


<hr>

<p><i> Written by <a href="http://kkrishna.in" target="_blank">Kundan Krishna</a> on 13th July 2016 </i></p>




</div></body>


</html>
